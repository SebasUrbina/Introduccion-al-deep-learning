# -*- coding: utf-8 -*-
"""Tarea_3_Intro_Deep_Learning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yW5N7iUdHhg-w0OYrraiLmhf-vojj2iU

# 🎁🎅**Tarea 3 - Introducción al Deep Learning** 🎅🎁
Integrantes: 
- Michael Clemans
- Paula Marín
- Bárbara Perez
- Sebastián Urbina

# Preparación

## Librerias

Primero se cargan todas las librerias que se usarán para esta pregunta.
"""

import pandas as pd
import seaborn as sns
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
from scipy.signal import convolve2d
import tensorflow as tf
from tensorflow import keras
import os
import keras.backend as K
import warnings
warnings.filterwarnings("ignore")

"""## Conectar drive

Se conecta con un drive para enlazar el contenido que se necesita.
"""

from google.colab import drive
drive.mount('/content/drive')

"""## Funciones útiles

Se define la siguiente función que se usará posteriormente para graficar el entrenamiento de los modelos
"""

def graficos_entreno(metricas, h):
    f, ax = plt.subplots(1,2,figsize=(14,4))
    k = 0
    for i in range(2):
        ax[i].plot(h.history[metricas[k]]);
        ax[i].plot(h.history['val_{}'.format(metricas[k])]);
        ax[i].set_ylabel(metricas[k])
        ax[i].legend(['train', 'val'], loc='upper left')
        ax[i].set_xlabel('epoch')
        k+=1
    plt.show()

"""Función extraida del auxiliar 9, que servirá posteriormente para graficar la data"""

def plotImages(images_arr):
    fig, axes = plt.subplots(1, 5, figsize=(20,20))
    axes = axes.flatten()
    for img, ax in zip(images_arr, axes):
        ax.imshow(img)
    plt.tight_layout()
    plt.show()

"""La siguiente función permite tomar una imagen del conjunto de entrenamiento al azar."""

import random
from random import sample
import matplotlib.image as mpimg
from PIL import Image
random.seed(1)
def img_sample(name, out_shape):
    """
    Toma una imagen al azar del conjunto de entrenamiento(train) de la categoria "name" y dimensiones "out_shape"
    """
    path = 'train/'+name #buscamos la imagen
    list_images = os.listdir(os.path.join(path)) #listamos todas las imagenes
    choice = sample(list_images, 1)[0] #elegimos una al azar
    img = Image.open(path+"/"+choice).resize((out_shape,out_shape)) #la abrimos y cambiamos el tamaño
    return img

"""La siguiente función grafica la imagen real versus la predicción de 2 modelos. Si la predicción es errónea se busca una imagen al azar de la predicción errónea para observar visualmente alguna idea de por qué se equivocó."""

def plot_prediction(img, label, modelo_sin_aug, modelo_con_aug):
    """
    Recibe una imagen(img) y una etiqueta(label) y 2 modelos. Grafica la imagen real versus las predicciones de los modelos.

    Si el modelo se equivoca buscar una imagen al azar de la predicción errónea.
    """
    fig, (ax1,ax2,ax3) = plt.subplots(1,3, figsize=(10,6)) #1 fila 3 columnas
    #Real
    real_name = test_labels[label[0].argmax()]
    #Prediction model sin data aug.
    predict_1 = modelo_sin_aug.predict(img).argmax(1)[0] #predecimos con el modelo sin data augmentation
    predicted_name_1 = test_labels[predict_1]
    #Prediction model con data aug.
    predict_2 = modelo_con_aug.predict(img).argmax(1)[0] #predecimos con el modelo con data augmentation 
    predicted_name_2 = test_labels[predict_2]

    #Graficamos la imagen real
    ax1.imshow(img[0])
    ax1.set_title(f'Real: {real_name}')

    if real_name == predicted_name_1: #Si la predicción coincide devolvemos la misma imagen
        ax2.imshow(img[0])
        ax2.set_title(f'Model without Data Aug.\nPredicted: {predicted_name_1}')   
    
    else:
        ax2.imshow(img_sample(predicted_name_1, IMG_SHAPE)) #Si la prediccion es erronea buscamos una imagen de la prediccion(erronea)
        ax2.set_title(f'Model without Data Aug.\nPredicted: {predicted_name_1}') 

    if real_name == predicted_name_2:
        ax3.imshow(img[0])
        ax3.set_title(f'Model with Data Aug.\nPredicted: {predicted_name_2}')   

    else:
        ax3.imshow(img_sample(predicted_name_2, IMG_SHAPE))
        ax3.set_title(f'Model with Data Aug.\nPredicted: {predicted_name_2}')

"""La siguiente función grafica la matriz de confusión(se utilizará para la P2) y un resumen de las predicciones."""

from sklearn.metrics import confusion_matrix
from mlxtend.plotting import plot_confusion_matrix
from sklearn.metrics import classification_report

def plot_cm(X_test, Y_test, model):
    y_true = Y_test
    y_pred = np.where( model.predict(X_test)>0.5, 1,0).reshape(-1)


    fig, ax = plot_confusion_matrix(conf_mat=confusion_matrix(y_true, y_pred),figsize=(8,8), show_absolute=True, show_normed=False, colorbar=False)
    plt.title('Matriz de confusión')
    plt.show()
    print(classification_report(y_true, y_pred))

"""# Pregunta 1

Se procede a cargar la data. `index` contiene la ruta a cada una de las imágenes y `metadata` información sobre los labels.
"""

base_path = '/content/drive/MyDrive/Colab Notebooks (1)/Intro al Deep Learning/Tarea_3/data_lego/'
#base_path = '/content/drive/MyDrive/Tarea_3/data_lego/'
index = pd.read_csv(base_path+'index.csv')
index_test = pd.read_csv(base_path+'test.csv')
metadata = pd.read_csv(base_path+'metadata.csv')

"""El `dataframe` `index` contiene la ruta a cada imagen con su respectivo label."""

index.head(3)

"""## Análisis exploratorio

Se muestras algunas imagenes aleatorias del dataset con su respectivo label y nombre
"""

import matplotlib.image as mpimg

n = 3
fig = plt.figure(figsize=(16,10))
for i, (path, label) in enumerate(index.sample(n**2).values):
    plt.subplot(n,n, i+1) 
    imagen = mpimg.imread(base_path + path)
    plt.xticks([]) #quitamos eje x
    plt.yticks([]) #quitamos eje y
    name = metadata[metadata['class_id']==label]['minifigure_name'].values.item() #Recuperamos el nombre de la figura segun su label
    plt.title(name +'({})'.format(label)) #Agregamos el titulo con el label entre parentesis
    plt.xlabel(f'Shape: {imagen.shape}')
    plt.imshow(imagen)

"""Se tienen imagenes de 512x512 pixeles con distintos personajes y distintos ángulos de captura.

A continuación se revisa el histograma del total de clases
"""

f, ax = plt.subplots(figsize=(12,6))
sns.histplot(index['class_id'],ax=ax, bins=37)
max_label = max(index['class_id'])
ax.set_xticks(list(range(1,max_label+1)))
plt.title('Histograma de categorias')
plt.xlabel('Categoria');

"""Se puede observar que hay 37 categorías, que cuentan con una mínima cantidad de ejemplos correspondiente a 7 para algunas y 12 para otras. En general no están tan desbalanceados los datos, sin embargo, en cantidad son pocos.

## Reordenar las imágenes

En esta sección se copiarán las imagenes desde las carpetas base a una llamada `train`, donde se creará una carpeta por categoría(label) y se iran guardando. Esto para poder utilizar posteriormente `ImageDataGenerator` de keras.
"""

import os
import shutil

try:
    os.mkdir('train') #Creamos una carpeta de train 
    os.mkdir('test')
except:
    print('Ya existe la carpeta train y test')

base_train = '/content/train/'
base_test = '/content/test/'

labels = pd.unique(index['class_id'])
#Copiamos la data en train
for label in labels: 
    class_name = metadata.loc[metadata['class_id']==label, 'minifigure_name'].values[0]
    try:
        os.mkdir(base_train+class_name) #Entramos a "train/" y creamos la carpeta respectiva del label
        os.mkdir(base_test+class_name)  #Entramos a "test/" y creamos la carpeta respectiva del label
    except:
        print(f'Carpeta {class_name} ya existe')
    label_dir_train = os.path.join(base_train, class_name)
    label_dir_test = os.path.join(base_test, class_name)
    for img_path in index[index['class_id']==label]['path']: #Recorremos cada imagen por carpeta
        shutil.copy(base_path + img_path, label_dir_train)
    
    for img_path in index_test[index_test['class_id']==label]['path']: #Recorremos las imagenes de la carpeta test
        shutil.copy(base_path + img_path, label_dir_test)

"""## Preprocesamiento

Se preprocesan los datos utilizando `ImageDataGenerator` de `Keras`, lo que hace el trabajo mucho más fácil. Asimismo, se crea un conjunto de validación correspondiente al 20% del de entrenamiento.

Cabe destacar que se utilizará `IMG_SHAPE=224` porque es la dimensión de entrada que recibe el modelo de base convolucional `MobileNetV2`.
"""

from tensorflow.keras.preprocessing.image import ImageDataGenerator

BATCH_SIZE = 32
IMG_SHAPE = 224

image_gen_train = ImageDataGenerator(rescale= 1./255, validation_split=0.2)


#Train
data_gen_train = image_gen_train.flow_from_directory(batch_size=BATCH_SIZE,
                                    directory=base_train, #directorio(/train)
                                    target_size=(IMG_SHAPE,IMG_SHAPE),
                                    shuffle=True,
                                    subset='training',
                                    seed=1)

#Validación
data_gen_val = image_gen_train.flow_from_directory(batch_size=BATCH_SIZE,
                                    directory=base_train, #directorio(/train)
                                    target_size=(IMG_SHAPE,IMG_SHAPE),
                                    shuffle=True,
                                    subset='validation',
                                    seed=1)

"""Adicionalmente se cargan los datos de testeo para comparar posteriormente ambos modelos"""

image_gen_test = ImageDataGenerator(rescale= 1./255)

#Testing
data_gen_test = image_gen_test.flow_from_directory(batch_size=1,
                                    directory=base_test,#directorio(/test)
                                    target_size=(IMG_SHAPE,IMG_SHAPE),
                                    shuffle=False)

"""Imprimir algunas imagenes del generador"""

fig = plt.figure(figsize=(16,10))
for img, label in data_gen_train:
    for i in range(5):
        plt.subplot(1,5,i+1)
        plt.xticks([])
        plt.yticks([])
        plt.imshow(img[i])
        plt.xlabel(f'Label: {np.argmax(label[i])}')
    break
plt.show()

"""## Modelo Sin Data augmentation"""

from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten, BatchNormalization, Activation

"""Se utilizará como base del modelo(feature extraction) un modelo pre-entrenado llamado MobileNetV2. Se utilizará sólo como base convolucional, por lo que se reentrenará una nueva cabecera."""

import tensorflow_hub as hub

url ="https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4" #descargamos el modelo
mobilenetv2 = hub.KerasLayer(url, input_shape=(IMG_SHAPE,IMG_SHAPE,3)) #lo guardamos

#Congelamos el modelo descargado
mobilenetv2.trainable = False

"""Se le agrega una capa densa de 512 neuronas y DropOut de 0.5. Asimismo en la salida se tiene una softmax sobre 37(cantidad clases) neuronas."""

import keras.backend as K
K.clear_session()

drop_out = 0.5
n_classes = data_gen_train.num_classes

modelo_sin_aug = Sequential([
                             mobilenetv2, #Red preentrenada
                             Dense(256, activation='relu'), #Fully connected de 512 neuronas
                             Dropout(drop_out),
                             Dense(256, activation='relu'), #Fully connected de 512 neuronas
                             Dropout(drop_out),
                             Dense(n_classes, activation='softmax')
])
modelo_sin_aug.summary()

"""Se utilizará la métrica de Accuracy para evaluar los mdoelos"""

modelo_sin_aug.compile(optimizer='adam',
                   loss='categorical_crossentropy',
                   metrics=['accuracy'])

"""Se procede a entregar agregando la callback de `checkpoint`, la cuál guardará el modelo con mejor `val_accuracy` durante el entrenamiento."""

N_EPOCHS = 100
early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', 
                                               patience=12,
                                               verbose=1)
checkpoint = keras.callbacks.ModelCheckpoint(filepath="best_modelo_sin_aug", 
                                             save_weights_only=True,
                                             monitor='val_accuracy', 
                                             mode='max', 
                                             save_best_only=True, 
                                             verbose=1)

history_modelo_sin_aug = modelo_sin_aug.fit(
    data_gen_train,
    steps_per_epoch=int(np.ceil(data_gen_train.samples // BATCH_SIZE)),
    epochs=N_EPOCHS,
    validation_data=data_gen_val,
    validation_steps=int(np.ceil(data_gen_val.samples // BATCH_SIZE)),
    callbacks=[early_stopping, checkpoint]
)

graficos_entreno( ['loss', 'accuracy'], history_modelo_sin_aug)

"""En los gráficos se observa un gap no tan grande, y se observa que el modelo aprende bastante en las primeras 20 epocas, donde el accuracy de validación aumenta. Luego de esto, comienza de alguna manera a overfittear y a aumentar el gap entre train y val.

Se cargan los pesos del mejor modelo guardado en base al `val_accuracy`
"""

modelo_sin_aug.load_weights('best_modelo_sin_aug')

"""## Modelo con Data Augmentation

Se generan nuevas imágenes, las cuales corresponden a modificaciones de las imágenes de entrenamiento de cada personaje, donde se aplica rotación, zoom y reflexión, para luego agregarlas al modelo y aumentar la cantidad de datos.
"""

BATCH_SIZE = 32
IMG_SHAPE = 224

image_gen_train_aug = ImageDataGenerator(rescale= 1./255,
                                     rotation_range=20,
                                     width_shift_range=0.2,
                                     height_shift_range=0.2,
                                     shear_range=5,
                                     zoom_range=0.2,
                                     validation_split=0.2)

image_gen_val_aug = ImageDataGenerator(rescale= 1./255,
                                       validation_split=0.2)
#Train
data_gen_train_aug = image_gen_train_aug.flow_from_directory(batch_size=BATCH_SIZE,
                                    directory=base_train,
                                    target_size=(IMG_SHAPE,IMG_SHAPE),
                                    subset='training',
                                    seed=1)

#Validation
data_gen_val_aug = image_gen_val_aug.flow_from_directory(batch_size=BATCH_SIZE,
                                    directory=base_train,
                                    target_size=(IMG_SHAPE,IMG_SHAPE),
                                    subset='validation',
                                    seed=1)

augmented_images = [data_gen_train_aug[6][0][0] for i in range(5)] #[class][img,label][batch_size]
plotImages(augmented_images)

drop_out = 0.5
n_classes = data_gen_train.num_classes

modelo_con_aug = Sequential([
                             mobilenetv2, #Red preentrenada
                             Dense(256, activation='relu'),
                             Dropout(drop_out),
                             Dense(256, activation='relu'),
                             Dropout(drop_out),
                             Dense(n_classes, activation='softmax')
])
modelo_con_aug.summary()

modelo_con_aug.compile(optimizer='adam',
                   loss='categorical_crossentropy',
                   metrics=['accuracy'])

N_EPOCHS = 100
early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', 
                                               patience=12,
                                               verbose=1)
checkpoint = keras.callbacks.ModelCheckpoint(filepath="best_modelo_con_aug", 
                                             save_weights_only=True,
                                             monitor='val_accuracy', 
                                             mode='max', 
                                             save_best_only=True, 
                                             verbose=1)
history_modelo_con_aug = modelo_con_aug.fit(
    data_gen_train_aug,
    steps_per_epoch=int(np.ceil(data_gen_train_aug.samples // BATCH_SIZE)),
    epochs=N_EPOCHS,
    validation_data=data_gen_val_aug,
    validation_steps=int(np.ceil(data_gen_val_aug.samples // BATCH_SIZE)),
    callbacks=[checkpoint, early_stopping]
)

graficos_entreno( ['loss', 'accuracy'], history_modelo_con_aug)

"""En ambos gráficos, correspondientes al de loss y al de accuracy se observa que el gap no es tan grande entre los datos de entrenamiento y datos de validación.
En el gráfico de loss se observa que hay una mayor perdida en los datos de validación, pero no es tan distante a la de entrenamiento. 
En el gráfico de accuracy, se observa que los datos de validación presenta una menor exactitud, aún así se acerca a un 80% lo cual no es malo.

Se cargan los pesos del mejor modelo guardado en base al `val_auc`
"""

modelo_con_aug.load_weights('best_modelo_con_aug')

"""## Comparación modelos"""

print('Accuracy modelo sin data augmentation: {:.2f}%'.format(100*modelo_sin_aug.evaluate(data_gen_test)[1]))

print('Accuracy modelo con data augmentation: {:.2f}%'.format(100*modelo_con_aug.evaluate(data_gen_test)[1]))

"""### Predicciones

Se define `test_labels` que no es nada mas que el diccionario de las clases "invertido" para poder acceder a cada nombre del personaje de lego directamente con el numero asociado.
"""

test_labels = {v: k for k,v in data_gen_test.class_indices.items()} 
test_labels

"""A continuación se observan algunas predicciones de los modelos entrenados. Esta usa la función `plot_prediction` definida en el apartado de funciones a utilizar."""

N = 10 #Predicciones a comprar

i = 0
for img, label in data_gen_test:
    plot_prediction(img, label, modelo_sin_aug, modelo_con_aug)
    i+=1
    if i==N:
        break

"""Se puede observar que en ciertos casos particulares se equivoca en la predicción, posiblemente porque tienen bastantes colores en común y el modelo no identifica completamente la figura.

## Conclusiones

El dataset trabajado en general tenía pocos datos y una gran cantidad de clases(37), sin embargo, estaban relativamente balanceadas. Es así, como luego de trabajar la data, observar las imágenes y dividirla en train, validacion y testeo usando la función ImageDataGenerator de Keras, se procedió a probar diferentes modelos.

Luego de entrenar a modo de prueba y error muchos modelos de diferentes arquitecturas(que para efectos prácticos no se explicitaron a lo largo del código) y no obtener resultados favorables(no se pasaba el 40% de accuracy en validación) se decidió usar un modelo pre-entrenado como base convolucional congelada. El cual corresponde a MobileNetV2 y posee más de 2M de parámetros. 

Se realizaron varias pruebas entre con/sin data augmentation aplicando estos últimos modelos, en primera instancia se probó sólo con una capa oculta de 512 neuronas, donde el modelo que arrojaba mejores resultados correspondía al sin data augmentation, lo cual era contraintuitivo en primera instancia porque al entregar datos con algunas modificaciones, a priori el modelo debería ser más robusto, sin embargo, se cree que al usar data augmentation y tener sólo una capa oculta de cierta manera el modelo se complejizaba y no alcanzaba a capturar toda la "nueva" información de las imágenes modificadas en la única capa. 

Ante lo anterior, se decidió agregar una capa oculta y probar distintas combinaciones de neuronas(128,256,512) y se llegó a que con 256 el modelo con data augmentation obtenia mejores resultados, llegando inclusó al 85.5% de accuracy en el conjunto de testeo.

Cabe destacar que el aplicar transfer-learning es súper util para situaciones con pocos datos, y especialmente para este situación, donde se tenían muchas clases. 
<!-- Si bien el modelo sin data augmentation tiene mayor exactitud, se observa en el gráfico que se produce un overfitting. En cambio, en el modelo con data augmentation, el cual es menos exacto, se acercan mayormente los valores de entrenamiento y validación, permitiendo que en realidad haya un aprendizaje profundo en vez de aprender de memoria los datos. -->

# Pregunta 2

## Cargar data
"""

!pip3 install pickle5

"""Se procede a cargar la data directamente desde el archivo entregado."""

import pickle5 as pickle

path = '/content/drive/MyDrive/Colab Notebooks (1)/Intro al Deep Learning/Tarea_3/dataVentanasEtiquetadas_500ms_v2.pickle'

with open(path, "rb") as fh:
  data = pickle.load(fh)

# import pickle
# path = '/content/drive/MyDrive/Colab Notebooks (1)/Intro al Deep Learning/Tarea_3/dataVentanasEtiquetadas_500ms_v2.pickle'

# data = pd.read_pickle(path)

print('Cantidad de instancias:', len(data))

"""Se tienen 4198 instancias, es decir, ventanas temporales que contienen diferentes datos como por ejemplo encefalograma(ecg) y frecuencia cardiaca(hr).

A continuación se observa una de las 4198 instancias y todos los datos que contiene.
"""

data.get(1)

"""Si seleccionamos la data del encefalograma(ecg)."""

print('Time-steps: ',len(data.get(1)['ecg']['data'])) #Ejemplo de datos de encefalograma
print(data.get(1)['ecg']['data'])

"""Se puede notar que las ventanas temporales tienen 51 valores. Osea, las secuencias son de 51 valores temporales. Lo que conincide con los explicado en el enunciado, pues, la frecuencia de medición es de 100[Hz] y se tienen ventanas de 500[ms].

Por cada paciente se tienen 51 secuencias de tiempo para cada uno de los 20 atributos

## Análisis exploratorio
"""

def plot_ventana(i):
    """ 
    Permite graficar la data de la i-ésima instancias(de las 4198)
    """
    fig, (ax1,ax2) = plt.subplots(1,2,figsize=(18,8));
    ax1.plot(data.get(i)['ecg']['data'], label='ecg') #se extrae la data de ecg
    ax1.plot(data.get(i)['eda']['data'], label='eda') #se extrae la data de eda
    ax1.plot(data.get(i)['eda']['phasic'], label='phasic') #se extrae la data de phasicc
    ax1.plot(data.get(i)['hr']['data'], label='hr') #se extrae la data de hr
    ax1.plot(data.get(i)['ppg']['data'], label='ppg') #se extrae la data de ppg
    ax1.plot(data.get(i)['skt']['data'], label='skt') #se extrae la data de skt
    ax1.legend()
    ax1.set_xlabel('Tiempo[ms]')
    for k in range(1,15):
        ax2.plot(data.get(i)['eeg'][f'ch{k}'], label=f'ch{k}') #se extrae la data de las 14 señales electroencefalográficas
    ax2.set_title('Señales electroencefalográficas')
    ax2.set_xlabel('Tiempo[ms]')
    ax2.legend()
    fig.suptitle('Label: {}'.format(data.get(i)['label_2']), fontsize=16)
    fig.tight_layout()
    fig.show()

plot_ventana(500) #Una ventana particular

plot_ventana(600) #ejemplo

plot_ventana(20) #otro ejemplo

plot_ventana(26) #otro ejemplo

"""Graficamente no se observan "patrones" que permitan diferenciar entre ambas clases, ya que en unos casos algunas señales permanecen estáticas a lo largo del tiempo y otras sufren variaciones y viceversa.

Se crea una función que permite extraer toda la data de cierto atributo(att) y retorna una lista con los valores.
"""

def extraer_att(att):
    """
    Extrae todos los datos de todas las ventanas de cierto atributo
    """
    aux = []
    for ventana in range(len(data)):
        for time_step in data.get(ventana)[att]['data']:
            aux.append(time_step) #Guardamos el time-step
    return aux

"""Se observará como distribuyen los datos del encefalograma para todas las instancias temporales."""

ecg = extraer_att('ecg')
fig, ax = plt.subplots(figsize=(10,8))
sns.histplot(ecg, ax=ax);
ax.set_title('Distribución data encefalograma')
fig.show()

"""Se puede notar que tiende a distribuir normal la medición del encefalograma para todas las ventanas temporales."""

hr = extraer_att('hr') #extraemos data frecuencia cardiaca
fig, ax = plt.subplots(figsize=(10,8))
sns.histplot(hr, ax=ax);
ax.set_title('Distribución data frecuencia cardiaca')
fig.show()

skt = extraer_att('skt') #extraemos data temperatura superficial
fig, ax = plt.subplots(figsize=(10,8))
sns.histplot(skt, ax=ax);
ax.set_title('Distribución data temperatura superficial')
fig.show()

"""En general las distribuciones tienen a distribuir normal, a excepción de la data de frecuencia cardiaca, la cual no tiene distribución clara.

## Preprocesamiento

Extraeremos la data del diccionario. Por lo que se recorrerán todas las ventanas y se extraerá la información necesaria para dejar los datos en el formato y dimensiones necesarias para entrenar la red.
"""

atrs = ['ecg','eda','hr','ppg','skt','eeg'] #atributos

X=[]
for j in range(len(data)):
    aux = []
    data_aux = data.get(j)
    for i, att in enumerate(atrs):
        if att=='eda': #Eda contiene data y phasic, por lo que rescatamos ambos valores
            aux.append(data_aux[att]['data'])
            aux.append(data_aux[att]['phasic'])
        elif att=='eeg': #Eeg contiene 14 canales, por lo que, los recorremos y extraemos la data
            for k in range(1,15):
                aux.append(data_aux[att][f'ch{k}'])
        else: 
           aux.append(data_aux[att]['data']) #Del resto se extrae la data
    X.append(aux)
X = np.array(X)

"""Se observan las dimensiones"""

X.shape

"""Se tienen 4198 ventanas con 20 features o características cada una y 51 pasos temporales. Sin embargo, Se requiere la data en dimensiones (samples, time-steps, features). Por lo que se debe intercambiar la dimension (1)(features) por (2)(time-steps)"""

X = np.swapaxes(X,1,2) #Cambiamos la dimension 1 por la 2.
print(X.shape)

"""Se crea el vector con los labels o el valor a predecir. Se extraen directamente desde la data. Se usará el `label_2`, por lo que se tratará como un problema de 2 clases."""

from tensorflow.keras.utils import to_categorical

Y = []
for i in range(len(data)): #0,...,4197
    Y.append(data.get(i)['label_2']-1) #Restamos 1 para que todos comiencen desde 0
Y = np.array(Y)
#Y = to_categorical(y) #Lo pasamos a forma vectorial para operarlo en la red

"""Si se observa la distribución de la variable a predecir"""

from collections import Counter
Counter(Y)

"""Finalmente la data tiene las siguientes dimensiones, tal como se requieren para entrenar una red recurrente."""

print(X.shape)
print(Y.shape)

"""## División en train, valid, test

Se procede a dividir la data en 70% train, 20% validación y 10% para testear finalmente.
"""

from sklearn.model_selection import train_test_split

X_train, X_val, Y_train, Y_val = train_test_split(X,Y, test_size=0.3, random_state=2) # 70% train, 30% val

X_val, X_test, Y_val, Y_test = train_test_split(X_val,Y_val, test_size=0.33, random_state=2) #Del 30% de val restante, queda 20% val y 10% test.

print('Train shape:',X_train.shape, Y_train.shape, '\t || % total: {:.2f}%'.format(100.*X_train.shape[0]/X.shape[0]))
print('Validation shape:',X_val.shape, Y_val.shape, '|| % total: {:.2f}%'.format(100.*X_val.shape[0]/X.shape[0]))
print('Test shape:',X_test.shape, Y_test.shape, '\t || % total: {:.2f}%'.format(100.*X_test.shape[0]/X.shape[0]))

"""##  Modelo 1. LSTM"""

import keras.backend as K
from keras.models import Sequential
from keras.layers import Dense, LSTM, CuDNNLSTM, Dropout, BatchNormalization, SimpleRNN, Conv1D, MaxPooling1D, Input

K.clear_session()

n_timesteps, n_features = X_train.shape[1], X_train.shape[2] #Dimensiones (51,20)
input_shape = (n_timesteps, n_features)

drop_out = 0.5
#Creamos el modelo
simple_lstm = Sequential([
                          Input(shape=input_shape),

                          CuDNNLSTM(128),
                          BatchNormalization(),

                          Dense(128, activation='relu'),
                          Dropout(drop_out),
                          Dense(1, activation='sigmoid')                 
])

simple_lstm.summary()

simple_lstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

early_stopping = tf.keras.callbacks.EarlyStopping(patience=10, verbose=1)
history_simple_lstm = simple_lstm.fit(X_train, Y_train, epochs=100, batch_size=64, validation_data=(X_val,Y_val), callbacks=[early_stopping])

graficos_entreno( ['loss', 'accuracy'], history_simple_lstm)

"""Se puede observar un overfitting evidente, pues, el loss de validación desde la epoca 7 comienza a aumentar.

#### Evaluación sobre el conjunto de test.
"""

plot_cm(X_test, Y_test, simple_lstm)

"""Se tiene un accuracy de 59% en el conjunto test.

## Modelo 2. CNN + Stack of LSTM

Ahora hará un stack de 2 LSTM y se agregará una `capa convolucional` de 32 filtros, para filtrar las series de tiempo y un`max pooling` de tamaño 2. Además, como medida de evitar overfitting se utilizará `BatchNormalization` en las capas intermedias y `DroPout` entre la capa densa de 128 neuronas y la de salida.
"""

#K.clear_session()

n_timesteps, n_features = X_train.shape[1], X_train.shape[2] #Dimensiones (51,20)
input_shape = (n_timesteps, n_features)
drop_out = 0.5

#Creamos el modelo
cnn_lstm = Sequential([
                          Input(shape=input_shape),

                          Conv1D(filters=64, kernel_size=5, strides=1, activation='relu', padding='same'),
                          MaxPooling1D(pool_size=2),

                          CuDNNLSTM(64, return_sequences=True),
                          BatchNormalization(),

                          CuDNNLSTM(64),
                          BatchNormalization(),     
                          
                          Dense(128, activation='relu'),
                          Dropout(drop_out),
                          Dense(1, activation='sigmoid')     
])
cnn_lstm.summary()

cnn_lstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

early_stopping = tf.keras.callbacks.EarlyStopping(patience=10, verbose=1)
history_cnn_lstm = cnn_lstm.fit(X_train, Y_train, epochs=100, batch_size=64, validation_data=(X_val,Y_val), callbacks=[early_stopping])

graficos_entreno( ['loss', 'accuracy'], history_cnn_lstm)

"""Se puede observar que el modelo se comienza a overfittear bastante rápido.

#### Evaluación sobre el conjunto de test.
"""

plot_cm(X_test, Y_test, cnn_lstm)

"""Se observa una sutil mejora, sin embargo, no significativa en el modelo.

## Modelo 3. CNN + Stack of LSTM(subconjunto de features)

Ahora, se probará el modelo anterior con un subconjunto de las señales, especificamente las 14 señales de encefalograma(eeg).

Los features fueron guardados en el siguiente orden
"""

features = ['ecg','eda','phasic','hr','ppg','skt','ch1','ch2','ch3','ch4','ch5','ch6','ch7','ch8','ch9','ch10','ch11','ch12','ch13','ch14']

"""Se van a utilizar sólo las señales encefalográficas. Las que corresponden a"""

features[6:]

"""Por lo que se procede a crear el subconujnto."""

idx = 6
X_train_subset = X_train[:,:,idx:]
X_val_subset = X_val[:,:,idx:]
X_test_subset = X_test[:,:,idx:]

"""Procedemos a reiniciar el modelo de CNN+LSTM y utilizarlo sobre este subconjunto de datos."""

K.clear_session()

n_timesteps, n_features = X_train_subset.shape[1], X_train_subset.shape[2] #Dimensiones (51,14)
input_shape = (n_timesteps, n_features)
drop_out = 0.5

#Creamos el modelo
cnn_lstm_v2 = Sequential([
                          Input(shape=input_shape),

                          Conv1D(filters=64, kernel_size=5, strides=1, activation='relu', padding='same'),
                          MaxPooling1D(pool_size=2),

                          CuDNNLSTM(64, return_sequences=True),
                          BatchNormalization(),

                          CuDNNLSTM(64),
                          BatchNormalization(),     
                          
                          Dense(128, activation='relu'),
                          Dropout(drop_out),
                          Dense(1, activation='sigmoid')     
])
#cnn_lstm_v2.summary()

cnn_lstm_v2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
early_stopping = tf.keras.callbacks.EarlyStopping(patience=10, verbose=1)
history_cnn_lstm_subset = cnn_lstm_v2.fit(X_train_subset, Y_train, epochs=100, batch_size=64, validation_data=(X_val_subset,Y_val), callbacks=[early_stopping])

graficos_entreno( ['loss', 'accuracy'], history_cnn_lstm_subset)

"""#### Evaluación sobre el conjunto de test."""

plot_cm(X_test_subset, Y_test, cnn_lstm_v2)

"""## Modelo 4. CNN + Stack of LSTM(subconjunto de features) con menor learning rate

Para esta ocasión se utilizará el Modelo 3, con un learning rate más bajo y se agregará más neuronas en la última capa densa.
"""

K.clear_session()

n_timesteps, n_features = X_train_subset.shape[1], X_train_subset.shape[2] #Dimensiones (51,14)
input_shape = (n_timesteps, n_features)
drop_out = 0.5

#Creamos el modelo
cnn_lstm_v3 = Sequential([
                          Input(shape=input_shape),

                          Conv1D(filters=64, kernel_size=5, strides=1, activation='relu', padding='same'),
                          MaxPooling1D(pool_size=2),

                          CuDNNLSTM(64, return_sequences=True),
                          BatchNormalization(),

                          CuDNNLSTM(64),
                          BatchNormalization(),     
                          
                          Dense(256, activation='relu'), #Pasamos de 128 a 256
                          Dropout(drop_out),
                          Dense(1, activation='sigmoid')     
])
#cnn_lstm_v3.summary()

adam = keras.optimizers.Adam(lr=0.00001) #Se disminuye el learning rate
cnn_lstm_v3.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])
early_stopping = tf.keras.callbacks.EarlyStopping(patience=10, verbose=1)
history_cnn_lstm_subset_v2 = cnn_lstm_v3.fit(X_train_subset, Y_train, epochs=100, batch_size=64, validation_data=(X_val_subset,Y_val), callbacks=[early_stopping])

graficos_entreno( ['loss', 'accuracy'], history_cnn_lstm_subset_v2)

"""#### Evaluación sobre el conjunto de test."""

plot_cm(X_test_subset, Y_test, cnn_lstm_v3)

"""## Modelo 5. CNN + Stack of LSTM bidireccionales (subconjunto de features)"""

from keras.layers import Bidirectional

K.clear_session()

n_timesteps, n_features = X_train_subset.shape[1], X_train_subset.shape[2] #Dimensiones (51,14)
input_shape = (n_timesteps, n_features)
drop_out = 0.5

#Creamos el modelo
cnn_lstm_v4 = Sequential([
                          Input(shape=input_shape),

                          Conv1D(filters=64, kernel_size=5, strides=1, activation='relu', padding='same'),
                          MaxPooling1D(pool_size=2),

                          Bidirectional(
                              CuDNNLSTM(64, return_sequences=True)
                              ),
                          BatchNormalization(),

                          Bidirectional(
                              CuDNNLSTM(64)
                              ),
                          BatchNormalization(),     
                          
                          Dense(256, activation='relu'), #Pasamos de 128 a 256
                          Dropout(drop_out),
                          Dense(1, activation='sigmoid')     
])
#cnn_lstm_v4.summary()

adam = keras.optimizers.Adam(lr=0.0001) #Se disminuye el learning rate
cnn_lstm_v4.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])
early_stopping = tf.keras.callbacks.EarlyStopping(patience=10, verbose=1)
history_cnn_lstm_subset_v3 = cnn_lstm_v4.fit(X_train_subset, Y_train, epochs=100, batch_size=64, validation_data=(X_val_subset,Y_val), callbacks=[early_stopping])

graficos_entreno( ['loss', 'accuracy'], history_cnn_lstm_subset_v3)

"""#### Evaluación sobre el conjunto de test."""

plot_cm(X_test_subset, Y_test, cnn_lstm_v4)

"""## Conclusiones

A modo de resumen, se probó una gran variedad de modelos de 2 clases. Más especificamente el primer modelo añadió solo una capa LSTM y obtuvo una val_accuracy 0.645. El segundo utilizó 2 capas LSTM, una capa convolucional de 32 filtros con max pooling y se obtuvo un 0.635 de val_acurracy. El tercer modelo probo el segundo modelo añadiendole un subconjunto de las señales, correspondientes a las 14 señales de encefalograma y obtuvo un val_acurracy de 0.638. El cuarto modelo construyo sobre el tercero, con un learning rate más bajo y más neuronas en la capa densa, obteniendo un val_acurracy de 0.644. Por último el quinto modelo tomó todo lo anterior, pero cambiando la LSTM por una del tipo bidireccional, esta vez obteniendo apenas un val_acurracy de 0.617. 

Según lo anterior, se concluye no haber obtenido resultados favorables (aquellos sobre 80% accuracy/val_accuracy) que pudieran permitir demostrar efectividad y aprendizaje real. Se probó con subconjuntos de features, cambiando hiperparámetros, agregando capas densas, agregando LSTM en serie y sin embargo no pariecese haber surtido mucho efecto.

Como propuesta de mejora se plantea la posibilidad de haber utilizado mejores arquitecturas a las exploradas en el curso, potencialmente habiendose tratado con información de una dificultad de procesamiento que se escapa de alcance de los modelos en cuestión.

<center>
    <h1>
        🎅🎁 ¡Felices fiestas de fin de año! 🎅🎁
    </h1>
</center>
"""