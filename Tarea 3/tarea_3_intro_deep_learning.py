# -*- coding: utf-8 -*-
"""Tarea_3_Intro_Deep_Learning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yW5N7iUdHhg-w0OYrraiLmhf-vojj2iU

# 游꾸游꾼**Tarea 3 - Introducci칩n al Deep Learning** 游꾼游꾸
Integrantes: 
- Michael Clemans
- Paula Mar칤n
- B치rbara Perez
- Sebasti치n Urbina

# Preparaci칩n

## Librerias

Primero se cargan todas las librerias que se usar치n para esta pregunta.
"""

import pandas as pd
import seaborn as sns
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
from scipy.signal import convolve2d
import tensorflow as tf
from tensorflow import keras
import os
import keras.backend as K
import warnings
warnings.filterwarnings("ignore")

"""## Conectar drive

Se conecta con un drive para enlazar el contenido que se necesita.
"""

from google.colab import drive
drive.mount('/content/drive')

"""## Funciones 칰tiles

Se define la siguiente funci칩n que se usar치 posteriormente para graficar el entrenamiento de los modelos
"""

def graficos_entreno(metricas, h):
    f, ax = plt.subplots(1,2,figsize=(14,4))
    k = 0
    for i in range(2):
        ax[i].plot(h.history[metricas[k]]);
        ax[i].plot(h.history['val_{}'.format(metricas[k])]);
        ax[i].set_ylabel(metricas[k])
        ax[i].legend(['train', 'val'], loc='upper left')
        ax[i].set_xlabel('epoch')
        k+=1
    plt.show()

"""Funci칩n extraida del auxiliar 9, que servir치 posteriormente para graficar la data"""

def plotImages(images_arr):
    fig, axes = plt.subplots(1, 5, figsize=(20,20))
    axes = axes.flatten()
    for img, ax in zip(images_arr, axes):
        ax.imshow(img)
    plt.tight_layout()
    plt.show()

"""La siguiente funci칩n permite tomar una imagen del conjunto de entrenamiento al azar."""

import random
from random import sample
import matplotlib.image as mpimg
from PIL import Image
random.seed(1)
def img_sample(name, out_shape):
    """
    Toma una imagen al azar del conjunto de entrenamiento(train) de la categoria "name" y dimensiones "out_shape"
    """
    path = 'train/'+name #buscamos la imagen
    list_images = os.listdir(os.path.join(path)) #listamos todas las imagenes
    choice = sample(list_images, 1)[0] #elegimos una al azar
    img = Image.open(path+"/"+choice).resize((out_shape,out_shape)) #la abrimos y cambiamos el tama침o
    return img

"""La siguiente funci칩n grafica la imagen real versus la predicci칩n de 2 modelos. Si la predicci칩n es err칩nea se busca una imagen al azar de la predicci칩n err칩nea para observar visualmente alguna idea de por qu칠 se equivoc칩."""

def plot_prediction(img, label, modelo_sin_aug, modelo_con_aug):
    """
    Recibe una imagen(img) y una etiqueta(label) y 2 modelos. Grafica la imagen real versus las predicciones de los modelos.

    Si el modelo se equivoca buscar una imagen al azar de la predicci칩n err칩nea.
    """
    fig, (ax1,ax2,ax3) = plt.subplots(1,3, figsize=(10,6)) #1 fila 3 columnas
    #Real
    real_name = test_labels[label[0].argmax()]
    #Prediction model sin data aug.
    predict_1 = modelo_sin_aug.predict(img).argmax(1)[0] #predecimos con el modelo sin data augmentation
    predicted_name_1 = test_labels[predict_1]
    #Prediction model con data aug.
    predict_2 = modelo_con_aug.predict(img).argmax(1)[0] #predecimos con el modelo con data augmentation 
    predicted_name_2 = test_labels[predict_2]

    #Graficamos la imagen real
    ax1.imshow(img[0])
    ax1.set_title(f'Real: {real_name}')

    if real_name == predicted_name_1: #Si la predicci칩n coincide devolvemos la misma imagen
        ax2.imshow(img[0])
        ax2.set_title(f'Model without Data Aug.\nPredicted: {predicted_name_1}')   
    
    else:
        ax2.imshow(img_sample(predicted_name_1, IMG_SHAPE)) #Si la prediccion es erronea buscamos una imagen de la prediccion(erronea)
        ax2.set_title(f'Model without Data Aug.\nPredicted: {predicted_name_1}') 

    if real_name == predicted_name_2:
        ax3.imshow(img[0])
        ax3.set_title(f'Model with Data Aug.\nPredicted: {predicted_name_2}')   

    else:
        ax3.imshow(img_sample(predicted_name_2, IMG_SHAPE))
        ax3.set_title(f'Model with Data Aug.\nPredicted: {predicted_name_2}')

"""La siguiente funci칩n grafica la matriz de confusi칩n(se utilizar치 para la P2) y un resumen de las predicciones."""

from sklearn.metrics import confusion_matrix
from mlxtend.plotting import plot_confusion_matrix
from sklearn.metrics import classification_report

def plot_cm(X_test, Y_test, model):
    y_true = Y_test
    y_pred = np.where( model.predict(X_test)>0.5, 1,0).reshape(-1)


    fig, ax = plot_confusion_matrix(conf_mat=confusion_matrix(y_true, y_pred),figsize=(8,8), show_absolute=True, show_normed=False, colorbar=False)
    plt.title('Matriz de confusi칩n')
    plt.show()
    print(classification_report(y_true, y_pred))

"""# Pregunta 1

Se procede a cargar la data. `index` contiene la ruta a cada una de las im치genes y `metadata` informaci칩n sobre los labels.
"""

base_path = '/content/drive/MyDrive/Colab Notebooks (1)/Intro al Deep Learning/Tarea_3/data_lego/'
#base_path = '/content/drive/MyDrive/Tarea_3/data_lego/'
index = pd.read_csv(base_path+'index.csv')
index_test = pd.read_csv(base_path+'test.csv')
metadata = pd.read_csv(base_path+'metadata.csv')

"""El `dataframe` `index` contiene la ruta a cada imagen con su respectivo label."""

index.head(3)

"""## An치lisis exploratorio

Se muestras algunas imagenes aleatorias del dataset con su respectivo label y nombre
"""

import matplotlib.image as mpimg

n = 3
fig = plt.figure(figsize=(16,10))
for i, (path, label) in enumerate(index.sample(n**2).values):
    plt.subplot(n,n, i+1) 
    imagen = mpimg.imread(base_path + path)
    plt.xticks([]) #quitamos eje x
    plt.yticks([]) #quitamos eje y
    name = metadata[metadata['class_id']==label]['minifigure_name'].values.item() #Recuperamos el nombre de la figura segun su label
    plt.title(name +'({})'.format(label)) #Agregamos el titulo con el label entre parentesis
    plt.xlabel(f'Shape: {imagen.shape}')
    plt.imshow(imagen)

"""Se tienen imagenes de 512x512 pixeles con distintos personajes y distintos 치ngulos de captura.

A continuaci칩n se revisa el histograma del total de clases
"""

f, ax = plt.subplots(figsize=(12,6))
sns.histplot(index['class_id'],ax=ax, bins=37)
max_label = max(index['class_id'])
ax.set_xticks(list(range(1,max_label+1)))
plt.title('Histograma de categorias')
plt.xlabel('Categoria');

"""Se puede observar que hay 37 categor칤as, que cuentan con una m칤nima cantidad de ejemplos correspondiente a 7 para algunas y 12 para otras. En general no est치n tan desbalanceados los datos, sin embargo, en cantidad son pocos.

## Reordenar las im치genes

En esta secci칩n se copiar치n las imagenes desde las carpetas base a una llamada `train`, donde se crear치 una carpeta por categor칤a(label) y se iran guardando. Esto para poder utilizar posteriormente `ImageDataGenerator` de keras.
"""

import os
import shutil

try:
    os.mkdir('train') #Creamos una carpeta de train 
    os.mkdir('test')
except:
    print('Ya existe la carpeta train y test')

base_train = '/content/train/'
base_test = '/content/test/'

labels = pd.unique(index['class_id'])
#Copiamos la data en train
for label in labels: 
    class_name = metadata.loc[metadata['class_id']==label, 'minifigure_name'].values[0]
    try:
        os.mkdir(base_train+class_name) #Entramos a "train/" y creamos la carpeta respectiva del label
        os.mkdir(base_test+class_name)  #Entramos a "test/" y creamos la carpeta respectiva del label
    except:
        print(f'Carpeta {class_name} ya existe')
    label_dir_train = os.path.join(base_train, class_name)
    label_dir_test = os.path.join(base_test, class_name)
    for img_path in index[index['class_id']==label]['path']: #Recorremos cada imagen por carpeta
        shutil.copy(base_path + img_path, label_dir_train)
    
    for img_path in index_test[index_test['class_id']==label]['path']: #Recorremos las imagenes de la carpeta test
        shutil.copy(base_path + img_path, label_dir_test)

"""## Preprocesamiento

Se preprocesan los datos utilizando `ImageDataGenerator` de `Keras`, lo que hace el trabajo mucho m치s f치cil. Asimismo, se crea un conjunto de validaci칩n correspondiente al 20% del de entrenamiento.

Cabe destacar que se utilizar치 `IMG_SHAPE=224` porque es la dimensi칩n de entrada que recibe el modelo de base convolucional `MobileNetV2`.
"""

from tensorflow.keras.preprocessing.image import ImageDataGenerator

BATCH_SIZE = 32
IMG_SHAPE = 224

image_gen_train = ImageDataGenerator(rescale= 1./255, validation_split=0.2)


#Train
data_gen_train = image_gen_train.flow_from_directory(batch_size=BATCH_SIZE,
                                    directory=base_train, #directorio(/train)
                                    target_size=(IMG_SHAPE,IMG_SHAPE),
                                    shuffle=True,
                                    subset='training',
                                    seed=1)

#Validaci칩n
data_gen_val = image_gen_train.flow_from_directory(batch_size=BATCH_SIZE,
                                    directory=base_train, #directorio(/train)
                                    target_size=(IMG_SHAPE,IMG_SHAPE),
                                    shuffle=True,
                                    subset='validation',
                                    seed=1)

"""Adicionalmente se cargan los datos de testeo para comparar posteriormente ambos modelos"""

image_gen_test = ImageDataGenerator(rescale= 1./255)

#Testing
data_gen_test = image_gen_test.flow_from_directory(batch_size=1,
                                    directory=base_test,#directorio(/test)
                                    target_size=(IMG_SHAPE,IMG_SHAPE),
                                    shuffle=False)

"""Imprimir algunas imagenes del generador"""

fig = plt.figure(figsize=(16,10))
for img, label in data_gen_train:
    for i in range(5):
        plt.subplot(1,5,i+1)
        plt.xticks([])
        plt.yticks([])
        plt.imshow(img[i])
        plt.xlabel(f'Label: {np.argmax(label[i])}')
    break
plt.show()

"""## Modelo Sin Data augmentation"""

from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten, BatchNormalization, Activation

"""Se utilizar치 como base del modelo(feature extraction) un modelo pre-entrenado llamado MobileNetV2. Se utilizar치 s칩lo como base convolucional, por lo que se reentrenar치 una nueva cabecera."""

import tensorflow_hub as hub

url ="https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4" #descargamos el modelo
mobilenetv2 = hub.KerasLayer(url, input_shape=(IMG_SHAPE,IMG_SHAPE,3)) #lo guardamos

#Congelamos el modelo descargado
mobilenetv2.trainable = False

"""Se le agrega una capa densa de 512 neuronas y DropOut de 0.5. Asimismo en la salida se tiene una softmax sobre 37(cantidad clases) neuronas."""

import keras.backend as K
K.clear_session()

drop_out = 0.5
n_classes = data_gen_train.num_classes

modelo_sin_aug = Sequential([
                             mobilenetv2, #Red preentrenada
                             Dense(256, activation='relu'), #Fully connected de 512 neuronas
                             Dropout(drop_out),
                             Dense(256, activation='relu'), #Fully connected de 512 neuronas
                             Dropout(drop_out),
                             Dense(n_classes, activation='softmax')
])
modelo_sin_aug.summary()

"""Se utilizar치 la m칠trica de Accuracy para evaluar los mdoelos"""

modelo_sin_aug.compile(optimizer='adam',
                   loss='categorical_crossentropy',
                   metrics=['accuracy'])

"""Se procede a entregar agregando la callback de `checkpoint`, la cu치l guardar치 el modelo con mejor `val_accuracy` durante el entrenamiento."""

N_EPOCHS = 100
early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', 
                                               patience=12,
                                               verbose=1)
checkpoint = keras.callbacks.ModelCheckpoint(filepath="best_modelo_sin_aug", 
                                             save_weights_only=True,
                                             monitor='val_accuracy', 
                                             mode='max', 
                                             save_best_only=True, 
                                             verbose=1)

history_modelo_sin_aug = modelo_sin_aug.fit(
    data_gen_train,
    steps_per_epoch=int(np.ceil(data_gen_train.samples // BATCH_SIZE)),
    epochs=N_EPOCHS,
    validation_data=data_gen_val,
    validation_steps=int(np.ceil(data_gen_val.samples // BATCH_SIZE)),
    callbacks=[early_stopping, checkpoint]
)

graficos_entreno( ['loss', 'accuracy'], history_modelo_sin_aug)

"""En los gr치ficos se observa un gap no tan grande, y se observa que el modelo aprende bastante en las primeras 20 epocas, donde el accuracy de validaci칩n aumenta. Luego de esto, comienza de alguna manera a overfittear y a aumentar el gap entre train y val.

Se cargan los pesos del mejor modelo guardado en base al `val_accuracy`
"""

modelo_sin_aug.load_weights('best_modelo_sin_aug')

"""## Modelo con Data Augmentation

Se generan nuevas im치genes, las cuales corresponden a modificaciones de las im치genes de entrenamiento de cada personaje, donde se aplica rotaci칩n, zoom y reflexi칩n, para luego agregarlas al modelo y aumentar la cantidad de datos.
"""

BATCH_SIZE = 32
IMG_SHAPE = 224

image_gen_train_aug = ImageDataGenerator(rescale= 1./255,
                                     rotation_range=20,
                                     width_shift_range=0.2,
                                     height_shift_range=0.2,
                                     shear_range=5,
                                     zoom_range=0.2,
                                     validation_split=0.2)

image_gen_val_aug = ImageDataGenerator(rescale= 1./255,
                                       validation_split=0.2)
#Train
data_gen_train_aug = image_gen_train_aug.flow_from_directory(batch_size=BATCH_SIZE,
                                    directory=base_train,
                                    target_size=(IMG_SHAPE,IMG_SHAPE),
                                    subset='training',
                                    seed=1)

#Validation
data_gen_val_aug = image_gen_val_aug.flow_from_directory(batch_size=BATCH_SIZE,
                                    directory=base_train,
                                    target_size=(IMG_SHAPE,IMG_SHAPE),
                                    subset='validation',
                                    seed=1)

augmented_images = [data_gen_train_aug[6][0][0] for i in range(5)] #[class][img,label][batch_size]
plotImages(augmented_images)

drop_out = 0.5
n_classes = data_gen_train.num_classes

modelo_con_aug = Sequential([
                             mobilenetv2, #Red preentrenada
                             Dense(256, activation='relu'),
                             Dropout(drop_out),
                             Dense(256, activation='relu'),
                             Dropout(drop_out),
                             Dense(n_classes, activation='softmax')
])
modelo_con_aug.summary()

modelo_con_aug.compile(optimizer='adam',
                   loss='categorical_crossentropy',
                   metrics=['accuracy'])

N_EPOCHS = 100
early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', 
                                               patience=12,
                                               verbose=1)
checkpoint = keras.callbacks.ModelCheckpoint(filepath="best_modelo_con_aug", 
                                             save_weights_only=True,
                                             monitor='val_accuracy', 
                                             mode='max', 
                                             save_best_only=True, 
                                             verbose=1)
history_modelo_con_aug = modelo_con_aug.fit(
    data_gen_train_aug,
    steps_per_epoch=int(np.ceil(data_gen_train_aug.samples // BATCH_SIZE)),
    epochs=N_EPOCHS,
    validation_data=data_gen_val_aug,
    validation_steps=int(np.ceil(data_gen_val_aug.samples // BATCH_SIZE)),
    callbacks=[checkpoint, early_stopping]
)

graficos_entreno( ['loss', 'accuracy'], history_modelo_con_aug)

"""En ambos gr치ficos, correspondientes al de loss y al de accuracy se observa que el gap no es tan grande entre los datos de entrenamiento y datos de validaci칩n.
En el gr치fico de loss se observa que hay una mayor perdida en los datos de validaci칩n, pero no es tan distante a la de entrenamiento. 
En el gr치fico de accuracy, se observa que los datos de validaci칩n presenta una menor exactitud, a칰n as칤 se acerca a un 80% lo cual no es malo.

Se cargan los pesos del mejor modelo guardado en base al `val_auc`
"""

modelo_con_aug.load_weights('best_modelo_con_aug')

"""## Comparaci칩n modelos"""

print('Accuracy modelo sin data augmentation: {:.2f}%'.format(100*modelo_sin_aug.evaluate(data_gen_test)[1]))

print('Accuracy modelo con data augmentation: {:.2f}%'.format(100*modelo_con_aug.evaluate(data_gen_test)[1]))

"""### Predicciones

Se define `test_labels` que no es nada mas que el diccionario de las clases "invertido" para poder acceder a cada nombre del personaje de lego directamente con el numero asociado.
"""

test_labels = {v: k for k,v in data_gen_test.class_indices.items()} 
test_labels

"""A continuaci칩n se observan algunas predicciones de los modelos entrenados. Esta usa la funci칩n `plot_prediction` definida en el apartado de funciones a utilizar."""

N = 10 #Predicciones a comprar

i = 0
for img, label in data_gen_test:
    plot_prediction(img, label, modelo_sin_aug, modelo_con_aug)
    i+=1
    if i==N:
        break

"""Se puede observar que en ciertos casos particulares se equivoca en la predicci칩n, posiblemente porque tienen bastantes colores en com칰n y el modelo no identifica completamente la figura.

## Conclusiones

El dataset trabajado en general ten칤a pocos datos y una gran cantidad de clases(37), sin embargo, estaban relativamente balanceadas. Es as칤, como luego de trabajar la data, observar las im치genes y dividirla en train, validacion y testeo usando la funci칩n ImageDataGenerator de Keras, se procedi칩 a probar diferentes modelos.

Luego de entrenar a modo de prueba y error muchos modelos de diferentes arquitecturas(que para efectos pr치cticos no se explicitaron a lo largo del c칩digo) y no obtener resultados favorables(no se pasaba el 40% de accuracy en validaci칩n) se decidi칩 usar un modelo pre-entrenado como base convolucional congelada. El cual corresponde a MobileNetV2 y posee m치s de 2M de par치metros. 

Se realizaron varias pruebas entre con/sin data augmentation aplicando estos 칰ltimos modelos, en primera instancia se prob칩 s칩lo con una capa oculta de 512 neuronas, donde el modelo que arrojaba mejores resultados correspond칤a al sin data augmentation, lo cual era contraintuitivo en primera instancia porque al entregar datos con algunas modificaciones, a priori el modelo deber칤a ser m치s robusto, sin embargo, se cree que al usar data augmentation y tener s칩lo una capa oculta de cierta manera el modelo se complejizaba y no alcanzaba a capturar toda la "nueva" informaci칩n de las im치genes modificadas en la 칰nica capa. 

Ante lo anterior, se decidi칩 agregar una capa oculta y probar distintas combinaciones de neuronas(128,256,512) y se lleg칩 a que con 256 el modelo con data augmentation obtenia mejores resultados, llegando inclus칩 al 85.5% de accuracy en el conjunto de testeo.

Cabe destacar que el aplicar transfer-learning es s칰per util para situaciones con pocos datos, y especialmente para este situaci칩n, donde se ten칤an muchas clases. 
<!-- Si bien el modelo sin data augmentation tiene mayor exactitud, se observa en el gr치fico que se produce un overfitting. En cambio, en el modelo con data augmentation, el cual es menos exacto, se acercan mayormente los valores de entrenamiento y validaci칩n, permitiendo que en realidad haya un aprendizaje profundo en vez de aprender de memoria los datos. -->

# Pregunta 2

## Cargar data
"""

!pip3 install pickle5

"""Se procede a cargar la data directamente desde el archivo entregado."""

import pickle5 as pickle

path = '/content/drive/MyDrive/Colab Notebooks (1)/Intro al Deep Learning/Tarea_3/dataVentanasEtiquetadas_500ms_v2.pickle'

with open(path, "rb") as fh:
  data = pickle.load(fh)

# import pickle
# path = '/content/drive/MyDrive/Colab Notebooks (1)/Intro al Deep Learning/Tarea_3/dataVentanasEtiquetadas_500ms_v2.pickle'

# data = pd.read_pickle(path)

print('Cantidad de instancias:', len(data))

"""Se tienen 4198 instancias, es decir, ventanas temporales que contienen diferentes datos como por ejemplo encefalograma(ecg) y frecuencia cardiaca(hr).

A continuaci칩n se observa una de las 4198 instancias y todos los datos que contiene.
"""

data.get(1)

"""Si seleccionamos la data del encefalograma(ecg)."""

print('Time-steps: ',len(data.get(1)['ecg']['data'])) #Ejemplo de datos de encefalograma
print(data.get(1)['ecg']['data'])

"""Se puede notar que las ventanas temporales tienen 51 valores. Osea, las secuencias son de 51 valores temporales. Lo que conincide con los explicado en el enunciado, pues, la frecuencia de medici칩n es de 100[Hz] y se tienen ventanas de 500[ms].

Por cada paciente se tienen 51 secuencias de tiempo para cada uno de los 20 atributos

## An치lisis exploratorio
"""

def plot_ventana(i):
    """ 
    Permite graficar la data de la i-칠sima instancias(de las 4198)
    """
    fig, (ax1,ax2) = plt.subplots(1,2,figsize=(18,8));
    ax1.plot(data.get(i)['ecg']['data'], label='ecg') #se extrae la data de ecg
    ax1.plot(data.get(i)['eda']['data'], label='eda') #se extrae la data de eda
    ax1.plot(data.get(i)['eda']['phasic'], label='phasic') #se extrae la data de phasicc
    ax1.plot(data.get(i)['hr']['data'], label='hr') #se extrae la data de hr
    ax1.plot(data.get(i)['ppg']['data'], label='ppg') #se extrae la data de ppg
    ax1.plot(data.get(i)['skt']['data'], label='skt') #se extrae la data de skt
    ax1.legend()
    ax1.set_xlabel('Tiempo[ms]')
    for k in range(1,15):
        ax2.plot(data.get(i)['eeg'][f'ch{k}'], label=f'ch{k}') #se extrae la data de las 14 se침ales electroencefalogr치ficas
    ax2.set_title('Se침ales electroencefalogr치ficas')
    ax2.set_xlabel('Tiempo[ms]')
    ax2.legend()
    fig.suptitle('Label: {}'.format(data.get(i)['label_2']), fontsize=16)
    fig.tight_layout()
    fig.show()

plot_ventana(500) #Una ventana particular

plot_ventana(600) #ejemplo

plot_ventana(20) #otro ejemplo

plot_ventana(26) #otro ejemplo

"""Graficamente no se observan "patrones" que permitan diferenciar entre ambas clases, ya que en unos casos algunas se침ales permanecen est치ticas a lo largo del tiempo y otras sufren variaciones y viceversa.

Se crea una funci칩n que permite extraer toda la data de cierto atributo(att) y retorna una lista con los valores.
"""

def extraer_att(att):
    """
    Extrae todos los datos de todas las ventanas de cierto atributo
    """
    aux = []
    for ventana in range(len(data)):
        for time_step in data.get(ventana)[att]['data']:
            aux.append(time_step) #Guardamos el time-step
    return aux

"""Se observar치 como distribuyen los datos del encefalograma para todas las instancias temporales."""

ecg = extraer_att('ecg')
fig, ax = plt.subplots(figsize=(10,8))
sns.histplot(ecg, ax=ax);
ax.set_title('Distribuci칩n data encefalograma')
fig.show()

"""Se puede notar que tiende a distribuir normal la medici칩n del encefalograma para todas las ventanas temporales."""

hr = extraer_att('hr') #extraemos data frecuencia cardiaca
fig, ax = plt.subplots(figsize=(10,8))
sns.histplot(hr, ax=ax);
ax.set_title('Distribuci칩n data frecuencia cardiaca')
fig.show()

skt = extraer_att('skt') #extraemos data temperatura superficial
fig, ax = plt.subplots(figsize=(10,8))
sns.histplot(skt, ax=ax);
ax.set_title('Distribuci칩n data temperatura superficial')
fig.show()

"""En general las distribuciones tienen a distribuir normal, a excepci칩n de la data de frecuencia cardiaca, la cual no tiene distribuci칩n clara.

## Preprocesamiento

Extraeremos la data del diccionario. Por lo que se recorrer치n todas las ventanas y se extraer치 la informaci칩n necesaria para dejar los datos en el formato y dimensiones necesarias para entrenar la red.
"""

atrs = ['ecg','eda','hr','ppg','skt','eeg'] #atributos

X=[]
for j in range(len(data)):
    aux = []
    data_aux = data.get(j)
    for i, att in enumerate(atrs):
        if att=='eda': #Eda contiene data y phasic, por lo que rescatamos ambos valores
            aux.append(data_aux[att]['data'])
            aux.append(data_aux[att]['phasic'])
        elif att=='eeg': #Eeg contiene 14 canales, por lo que, los recorremos y extraemos la data
            for k in range(1,15):
                aux.append(data_aux[att][f'ch{k}'])
        else: 
           aux.append(data_aux[att]['data']) #Del resto se extrae la data
    X.append(aux)
X = np.array(X)

"""Se observan las dimensiones"""

X.shape

"""Se tienen 4198 ventanas con 20 features o caracter칤sticas cada una y 51 pasos temporales. Sin embargo, Se requiere la data en dimensiones (samples, time-steps, features). Por lo que se debe intercambiar la dimension (1)(features) por (2)(time-steps)"""

X = np.swapaxes(X,1,2) #Cambiamos la dimension 1 por la 2.
print(X.shape)

"""Se crea el vector con los labels o el valor a predecir. Se extraen directamente desde la data. Se usar치 el `label_2`, por lo que se tratar치 como un problema de 2 clases."""

from tensorflow.keras.utils import to_categorical

Y = []
for i in range(len(data)): #0,...,4197
    Y.append(data.get(i)['label_2']-1) #Restamos 1 para que todos comiencen desde 0
Y = np.array(Y)
#Y = to_categorical(y) #Lo pasamos a forma vectorial para operarlo en la red

"""Si se observa la distribuci칩n de la variable a predecir"""

from collections import Counter
Counter(Y)

"""Finalmente la data tiene las siguientes dimensiones, tal como se requieren para entrenar una red recurrente."""

print(X.shape)
print(Y.shape)

"""## Divisi칩n en train, valid, test

Se procede a dividir la data en 70% train, 20% validaci칩n y 10% para testear finalmente.
"""

from sklearn.model_selection import train_test_split

X_train, X_val, Y_train, Y_val = train_test_split(X,Y, test_size=0.3, random_state=2) # 70% train, 30% val

X_val, X_test, Y_val, Y_test = train_test_split(X_val,Y_val, test_size=0.33, random_state=2) #Del 30% de val restante, queda 20% val y 10% test.

print('Train shape:',X_train.shape, Y_train.shape, '\t || % total: {:.2f}%'.format(100.*X_train.shape[0]/X.shape[0]))
print('Validation shape:',X_val.shape, Y_val.shape, '|| % total: {:.2f}%'.format(100.*X_val.shape[0]/X.shape[0]))
print('Test shape:',X_test.shape, Y_test.shape, '\t || % total: {:.2f}%'.format(100.*X_test.shape[0]/X.shape[0]))

"""##  Modelo 1. LSTM"""

import keras.backend as K
from keras.models import Sequential
from keras.layers import Dense, LSTM, CuDNNLSTM, Dropout, BatchNormalization, SimpleRNN, Conv1D, MaxPooling1D, Input

K.clear_session()

n_timesteps, n_features = X_train.shape[1], X_train.shape[2] #Dimensiones (51,20)
input_shape = (n_timesteps, n_features)

drop_out = 0.5
#Creamos el modelo
simple_lstm = Sequential([
                          Input(shape=input_shape),

                          CuDNNLSTM(128),
                          BatchNormalization(),

                          Dense(128, activation='relu'),
                          Dropout(drop_out),
                          Dense(1, activation='sigmoid')                 
])

simple_lstm.summary()

simple_lstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

early_stopping = tf.keras.callbacks.EarlyStopping(patience=10, verbose=1)
history_simple_lstm = simple_lstm.fit(X_train, Y_train, epochs=100, batch_size=64, validation_data=(X_val,Y_val), callbacks=[early_stopping])

graficos_entreno( ['loss', 'accuracy'], history_simple_lstm)

"""Se puede observar un overfitting evidente, pues, el loss de validaci칩n desde la epoca 7 comienza a aumentar.

#### Evaluaci칩n sobre el conjunto de test.
"""

plot_cm(X_test, Y_test, simple_lstm)

"""Se tiene un accuracy de 59% en el conjunto test.

## Modelo 2. CNN + Stack of LSTM

Ahora har치 un stack de 2 LSTM y se agregar치 una `capa convolucional` de 32 filtros, para filtrar las series de tiempo y un`max pooling` de tama침o 2. Adem치s, como medida de evitar overfitting se utilizar치 `BatchNormalization` en las capas intermedias y `DroPout` entre la capa densa de 128 neuronas y la de salida.
"""

#K.clear_session()

n_timesteps, n_features = X_train.shape[1], X_train.shape[2] #Dimensiones (51,20)
input_shape = (n_timesteps, n_features)
drop_out = 0.5

#Creamos el modelo
cnn_lstm = Sequential([
                          Input(shape=input_shape),

                          Conv1D(filters=64, kernel_size=5, strides=1, activation='relu', padding='same'),
                          MaxPooling1D(pool_size=2),

                          CuDNNLSTM(64, return_sequences=True),
                          BatchNormalization(),

                          CuDNNLSTM(64),
                          BatchNormalization(),     
                          
                          Dense(128, activation='relu'),
                          Dropout(drop_out),
                          Dense(1, activation='sigmoid')     
])
cnn_lstm.summary()

cnn_lstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

early_stopping = tf.keras.callbacks.EarlyStopping(patience=10, verbose=1)
history_cnn_lstm = cnn_lstm.fit(X_train, Y_train, epochs=100, batch_size=64, validation_data=(X_val,Y_val), callbacks=[early_stopping])

graficos_entreno( ['loss', 'accuracy'], history_cnn_lstm)

"""Se puede observar que el modelo se comienza a overfittear bastante r치pido.

#### Evaluaci칩n sobre el conjunto de test.
"""

plot_cm(X_test, Y_test, cnn_lstm)

"""Se observa una sutil mejora, sin embargo, no significativa en el modelo.

## Modelo 3. CNN + Stack of LSTM(subconjunto de features)

Ahora, se probar치 el modelo anterior con un subconjunto de las se침ales, especificamente las 14 se침ales de encefalograma(eeg).

Los features fueron guardados en el siguiente orden
"""

features = ['ecg','eda','phasic','hr','ppg','skt','ch1','ch2','ch3','ch4','ch5','ch6','ch7','ch8','ch9','ch10','ch11','ch12','ch13','ch14']

"""Se van a utilizar s칩lo las se침ales encefalogr치ficas. Las que corresponden a"""

features[6:]

"""Por lo que se procede a crear el subconujnto."""

idx = 6
X_train_subset = X_train[:,:,idx:]
X_val_subset = X_val[:,:,idx:]
X_test_subset = X_test[:,:,idx:]

"""Procedemos a reiniciar el modelo de CNN+LSTM y utilizarlo sobre este subconjunto de datos."""

K.clear_session()

n_timesteps, n_features = X_train_subset.shape[1], X_train_subset.shape[2] #Dimensiones (51,14)
input_shape = (n_timesteps, n_features)
drop_out = 0.5

#Creamos el modelo
cnn_lstm_v2 = Sequential([
                          Input(shape=input_shape),

                          Conv1D(filters=64, kernel_size=5, strides=1, activation='relu', padding='same'),
                          MaxPooling1D(pool_size=2),

                          CuDNNLSTM(64, return_sequences=True),
                          BatchNormalization(),

                          CuDNNLSTM(64),
                          BatchNormalization(),     
                          
                          Dense(128, activation='relu'),
                          Dropout(drop_out),
                          Dense(1, activation='sigmoid')     
])
#cnn_lstm_v2.summary()

cnn_lstm_v2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
early_stopping = tf.keras.callbacks.EarlyStopping(patience=10, verbose=1)
history_cnn_lstm_subset = cnn_lstm_v2.fit(X_train_subset, Y_train, epochs=100, batch_size=64, validation_data=(X_val_subset,Y_val), callbacks=[early_stopping])

graficos_entreno( ['loss', 'accuracy'], history_cnn_lstm_subset)

"""#### Evaluaci칩n sobre el conjunto de test."""

plot_cm(X_test_subset, Y_test, cnn_lstm_v2)

"""## Modelo 4. CNN + Stack of LSTM(subconjunto de features) con menor learning rate

Para esta ocasi칩n se utilizar치 el Modelo 3, con un learning rate m치s bajo y se agregar치 m치s neuronas en la 칰ltima capa densa.
"""

K.clear_session()

n_timesteps, n_features = X_train_subset.shape[1], X_train_subset.shape[2] #Dimensiones (51,14)
input_shape = (n_timesteps, n_features)
drop_out = 0.5

#Creamos el modelo
cnn_lstm_v3 = Sequential([
                          Input(shape=input_shape),

                          Conv1D(filters=64, kernel_size=5, strides=1, activation='relu', padding='same'),
                          MaxPooling1D(pool_size=2),

                          CuDNNLSTM(64, return_sequences=True),
                          BatchNormalization(),

                          CuDNNLSTM(64),
                          BatchNormalization(),     
                          
                          Dense(256, activation='relu'), #Pasamos de 128 a 256
                          Dropout(drop_out),
                          Dense(1, activation='sigmoid')     
])
#cnn_lstm_v3.summary()

adam = keras.optimizers.Adam(lr=0.00001) #Se disminuye el learning rate
cnn_lstm_v3.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])
early_stopping = tf.keras.callbacks.EarlyStopping(patience=10, verbose=1)
history_cnn_lstm_subset_v2 = cnn_lstm_v3.fit(X_train_subset, Y_train, epochs=100, batch_size=64, validation_data=(X_val_subset,Y_val), callbacks=[early_stopping])

graficos_entreno( ['loss', 'accuracy'], history_cnn_lstm_subset_v2)

"""#### Evaluaci칩n sobre el conjunto de test."""

plot_cm(X_test_subset, Y_test, cnn_lstm_v3)

"""## Modelo 5. CNN + Stack of LSTM bidireccionales (subconjunto de features)"""

from keras.layers import Bidirectional

K.clear_session()

n_timesteps, n_features = X_train_subset.shape[1], X_train_subset.shape[2] #Dimensiones (51,14)
input_shape = (n_timesteps, n_features)
drop_out = 0.5

#Creamos el modelo
cnn_lstm_v4 = Sequential([
                          Input(shape=input_shape),

                          Conv1D(filters=64, kernel_size=5, strides=1, activation='relu', padding='same'),
                          MaxPooling1D(pool_size=2),

                          Bidirectional(
                              CuDNNLSTM(64, return_sequences=True)
                              ),
                          BatchNormalization(),

                          Bidirectional(
                              CuDNNLSTM(64)
                              ),
                          BatchNormalization(),     
                          
                          Dense(256, activation='relu'), #Pasamos de 128 a 256
                          Dropout(drop_out),
                          Dense(1, activation='sigmoid')     
])
#cnn_lstm_v4.summary()

adam = keras.optimizers.Adam(lr=0.0001) #Se disminuye el learning rate
cnn_lstm_v4.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])
early_stopping = tf.keras.callbacks.EarlyStopping(patience=10, verbose=1)
history_cnn_lstm_subset_v3 = cnn_lstm_v4.fit(X_train_subset, Y_train, epochs=100, batch_size=64, validation_data=(X_val_subset,Y_val), callbacks=[early_stopping])

graficos_entreno( ['loss', 'accuracy'], history_cnn_lstm_subset_v3)

"""#### Evaluaci칩n sobre el conjunto de test."""

plot_cm(X_test_subset, Y_test, cnn_lstm_v4)

"""## Conclusiones

A modo de resumen, se prob칩 una gran variedad de modelos de 2 clases. M치s especificamente el primer modelo a침adi칩 solo una capa LSTM y obtuvo una val_accuracy 0.645. El segundo utiliz칩 2 capas LSTM, una capa convolucional de 32 filtros con max pooling y se obtuvo un 0.635 de val_acurracy. El tercer modelo probo el segundo modelo a침adiendole un subconjunto de las se침ales, correspondientes a las 14 se침ales de encefalograma y obtuvo un val_acurracy de 0.638. El cuarto modelo construyo sobre el tercero, con un learning rate m치s bajo y m치s neuronas en la capa densa, obteniendo un val_acurracy de 0.644. Por 칰ltimo el quinto modelo tom칩 todo lo anterior, pero cambiando la LSTM por una del tipo bidireccional, esta vez obteniendo apenas un val_acurracy de 0.617. 

Seg칰n lo anterior, se concluye no haber obtenido resultados favorables (aquellos sobre 80% accuracy/val_accuracy) que pudieran permitir demostrar efectividad y aprendizaje real. Se prob칩 con subconjuntos de features, cambiando hiperpar치metros, agregando capas densas, agregando LSTM en serie y sin embargo no pariecese haber surtido mucho efecto.

Como propuesta de mejora se plantea la posibilidad de haber utilizado mejores arquitecturas a las exploradas en el curso, potencialmente habiendose tratado con informaci칩n de una dificultad de procesamiento que se escapa de alcance de los modelos en cuesti칩n.

<center>
    <h1>
        游꾼游꾸 춰Felices fiestas de fin de a침o! 游꾼游꾸
    </h1>
</center>
"""